---
title: Надежная передача голосов
---

Голоса валидатора - это сообщения, имеющие критическую функцию для достижения согласия и непрерывной работы сети. Поэтому очень важно, чтобы они были надежно доставлены и закодированы в реестр.

## Проблемы

1. Ротация лидеров запускается PoH, то есть часами с большим дрейфом. Так много узлов, скорее всего, будут иметь неверное представление, активен ли следующий лидер в реальном времени или нет.
2. Следующий лидер можно легко быть переполнен. Таким образом, DDOS не только предотвратит доставку обычных транзакций, но и консенсусных сообщений.
3. UDP является ненадежным, и наш асинхронный протокол требует, чтобы любое отправленное сообщение передавалась повторно до тех пор, пока оно не будет замечено в реестре. Повторная передача может привести к непреднамеренной _проблеме громоподобного стада_ в отношении лидера с большим количеством валидаторов. В худшем случае флуд будет `(num_nodes * num_retransmits)`.
4. Отслеживание, что голос был передан через реестр, не гарантирует, что он появится в подтвержденном блоке. Текущий наблюдаемый блок может быть развернут. Валидаторы должны будут поддерживать состояние для каждого голоса и форка.

## Дизайн

1. Отправлять голоса как push-сообщение через gossip. Это гарантирует передачу голоса всем следующим лидерам, а не только непосредственного следующему.
2. Лидеры будут читать таблицу Crds для новых голосов и кодировать любые новые полученные голоса в блоки, которые они предлагают. Это позволяет включать голоса валидатора в форки отката всех будущих лидеров.
3. Валидаторы, которые получают голоса в реестре, добавят их в свою локальную таблицу crds не как push-запрос, а просто добавят их в таблицу. Это сокращает протокол push-сообщений, поэтому сообщения проверки не нужно повторно передавать дважды по сети.
4. CrdsValue для голосов должно выглядеть как `Votes(Vec<Transaction>)`

Каждая транзакция голосования должна содержать `wallclock` в своих данных. Стратегия слияния для голосов сохранит последние N наборов голосов в соответствии с настройками локального клиента. Для push/pull вектор проходит рекурсивно и каждая транзакция обрабатывается как отдельное CrdsValue со своими локальными часами и подписью.

Gossip разработан для эффективного распространения состояния. Сообщения, которые отправляются через Gossip-push, группируются и распространяются с минимальным остовным деревом на остальную часть сети. Любые частичные сбои в дереве активно устраняются с помощью протокола gossip-pull, минимизируя при этом объем данных, передаваемых между любыми узлами.

## Как этот дизайн решает проблемы

1. Поскольку у валидаторов нет простого способа синхронизироваться с лидерами в «активном» состоянии лидера, gossip позволяют в конечном итоге производить возможную доставку независимо от этого состояния.
2. Gossip доставит сообщения всем последующим лидерам, поэтому, если текущий лидер будет под флудом, следующий лидер уже получит эти голоса и сможет их закодировать.
3. Gossip минимизирует количество запросов через сеть, поддерживая эффективное остовное дерево и используя фильтры Блума для восстановления состояния. Так что повторная передача отката не является обязательной и сообщения группируются.
4. Лидеры, которые прочитали таблицу crds для голосований, будут кодировать все новые действительные голоса, которые появятся в таблице. Даже если блок этого лидера развернут, следующий лидер попытается добавить те же голоса без какой-либо дополнительной работы со стороны валидатора. Таким образом обеспечивается не только конечная доставка, но и конечное кодирование в реестр.

## Производительность

1. В худшем случае время распространения до следующего лидера - это Log\(N\) переходов с базой, зависящей от разветвления. При нашем текущем разветвлении по умолчанию равном 6, это примерно 6 переходов на 20 тысяч нод.
2. Лидер должен получить 20 тысяч голосов валидации, агрегированных gossip-push в шреды размером с MTU. Это уменьшит количество пакетов для сети с 20 000 до 80 шредов.
3. Каждый голос валидаторов реплицируется по всей сети. Для поддержания очереди из 5 предыдущих голосов таблица Crds увеличится на 25 мегабайт. `(20 000 узлов * 256 байт * 5)`.

## Двухступенчатое внедрение

Первоначально сеть может работать надежно с помощью всего 1 голоса, передаваемого и поддерживаемого через сеть с текущей реализацией голосования. Для небольших сетей разветвления 6 достаточно. В небольшой сети накладные расходы на память и push незначительны.

### Сеть валидаторов до 1k

1. Crds просто поддерживает последнее голосование валидаторов.
2. Голоса отправляются и передаются повторно независимо от того, отображаются ли они в реестре.
3. Фанаут 6.
4. В худшем случае 256kb накладных расходов памяти на узел.
5. В худшем случае 4 перехода для распространения на каждый узел.
6. Лидер должен получить весь набор голосов валидатора в 4 шредах push-сообщений.

### Сеть валидаторов до 20k

Все, что выше плюс следующее:

1. Таблица CRDS поддерживает вектор из 5 последних голосов валидаторов.
2. Голоса кодируют wallclock. CrdsValue::Votes - это тип, который повторяет в векторе транзакции для всех gossip протоколов.
3. Увеличение фанаута до 20.
4. В худшем случае 25mb накладных расходов памяти на узел.
5. До 4 переходов в худшем случае для доставки по всей сети.
6. 80 шредов, полученных лидером за все сообщения валидатора.
