# Leader Rotation

At any given moment, a cluster expects only one fullnode to produce ledger
entries. By having only one leader at a time, all validators are able to replay
identical copies of the ledger. The drawback of only one leader at a time,
however, is that a malicious leader is capable of censoring votes and
transactions. Since censoring cannot be distinguished from the network dropping
packets, the cluster cannot simply elect a single node to hold the leader role
indefinitely. Instead, the cluster minimizes the influence of a malicious
leader by rotating which node takes the lead.

Each validator selects the expected leader using the same algorithm, described
below. When the validator receives a new signed ledger entry, it can be certain
that entry was produced by the expected leader.  The order of slots which each
leader is assigned a slot is called a *Leader Schedule*.

## Leader Schedule Rotation

A block's slot is valid if it is generated by the leader computed from a *Leader
Schedule* in the block that crossed the epoch boundary at least 1 epoch ago.

A leader schedule is generated every epoch when a block crosses the epoch
boundary.  While operating without partitions lasting longer than an epoch the
schedule only needs to be generated when the root fork crosses the epoch
boundary.

The Leader Schedule is active at the next epoch.  The epoch should be much
longer then the amount of time required for a block to be committed as root.
Such that the Leader Schedule is based on a block that has been committed as the
root fork.  Since the schedule is for the next epoch, any new stakes committed
to the root fork will not be active until the next epoch.

Without a partition lasting longer then an epoch, the cluster will work as follows:

1. A validator continuously updates its own root fork as it votes.

2. The validator updates its leader schedule each time the slot height crosses
an epoch boundary.

For example:

The epoch duration is 100 slots. The root fork is updated from fork computed at
slot height 99 to a fork computed at slot height 102. Forks with slots at height
100,101 were skipped because of failures.  The new leader schedule is computed
using fork at slot height 102.  It is active from slot 200 until it is updated
again.

No inconsistency can exist because every validator that is voting with the
cluster has skipped 100 and 101 when its root reaches 102.

### Leader Schedule Rotation with Epoch Sized Partitions.

Consider the following scenario:

Two partitions that are generating half of the blocks each.  Neither is coming
to a definitive supermajority fork.  Both will cross epoch 100 and 200 without
actually committing to a root and therefore a cluster wide commitment to a new
Leader Schedule.

In this unstable scenario, multiple valid leader schedules exist.

* An Leader Schedule is generated for every fork whose direct parent is in the
previous epoch.

* The Leader Schedule is valid after the start of the next epoch for descendant
forks until it is updated.

Each partition's schedule will diverge after the partition lasts more than an
epoch.  For this reason, the epoch duration should be selected to be much much
larger then slot time and the expected length for a fork to be committed to
root.

## Leader Schedule Generation at Genesis

The genesis block declares the first leader.  This leader is scheduled for the
first two epochs.  The length of the first two epochs can be specified in the
genesis block as well.  The minimum length of the first epochs must be greater
than or equal than the maximum rollback depth as defined in [fork
selection](fork-selection.md).

## Leader Schedule Generation Algorithm

Leader schedule is generated using a predefined seed.  The process is as follows:

1. Periodically use the PoH tick height (a monotonically increasing counter) to
   seed a stable pseudo-random algorithm.
2. At that height, sample the bank for all the staked accounts with leader
   identities that have voted within a cluster-configured number of ticks. The
   sample is called the *active set*.
3. Sort the active set by stake weight.
4. Use the random seed to select nodes weighted by stake to create a
   stake-weighted ordering.
5. This ordering becomes valid after a cluster-configured number of ticks.

## Schedule Attack Vectors

### Seed

The seed that is selected is predictable but unbiasable.  There is no grinding
attack to influence its outcome. 

### Active Set

The active set, however, can be biased by a leader by censoring validator votes.
To reduce the likelihood of censorship, the active set is sampled many slots in
advance, such that votes will have been collected by multiple leaders. If even
one node is honest, the malicious leaders will not be able to use censorship to
influence the leader schedule.

### Staking

Leaders can censor new staking transactions, or refuse to validate blocks with
new stakes.

## Appending Entries

The lifetime of a leader schedule is called an *epoch*. The epoch is split into
*slots*, where each slot has a duration of `T` PoH ticks.

A leader transmits entries during its slot.  After `T` ticks, all the
validators switch to the next scheduled leader. Validators must ignore entries
sent outside a leader's assigned slot.

All `T` ticks must be observed by the next leader for it to build its own
entries on. If entries are not observed (leader is down) or entries are invalid
(leader is buggy or malicious), the next leader must produce ticks to fill the
previous leader's slot. Note that the next leader should do repair requests in
parallel, and postpone sending ticks until it is confident other validators
also failed to observe the previous leader's entries. If a leader incorrectly
builds on its own ticks, the leader following it must replace all its ticks.

## Alternative Leader Scheduler Rotation

Two phase commit:

1. Root crosses the epoch boundary.

2. An *activate* transaction is submitted by the leader to acknowledge the new
schedule if they observe a supermajority at root.

3. The *activate* transaction sets the activation for the new Leader Schedule to
be at the epoch following block's slot plus half an epoch length.  So any forks in
the next epoch boundary use the new Leader Schedule.

Validators reject the block if they do not observe the root to be past the epoch
boundary.  The complexity with this approach revolves around the incentives for
any leader to bet that the super majority has confirmed the root.  One possible
solution is that *all* leader rewards are not generated until this transaction
is encoded into the ledger.
