# Leader Duplicate Block Slashing

This design describes how the cluster slashes leaders that produce
duplicate blocks.

Leaders that produce multiple blocks for the same slot increase the
number of potential forks that the cluster has to resolve.

## Shred Format

Shreds are produced by leaders during their scheduled slot.  Each
shred is signed by the leader and is transmitted to the cluster via
Turbine. A shred contains the following

* signature
* header: slot index, shred index
* message data

The signature is of the merkle tree of the shred data.

```text
              merke root
    /                        \
(slot index, shred index)        hash(msg)
```

## Proof of a Duplicate Shred

Any two different signatures that show a merkle path to the same
`(slot index, shred index)` for the same leader are proof that the
leader produced two conflicting blocks.

## Storing Multiple Versions of the Same Slot
When there is a possibility of duplicate blocks, a validator needs to track and
replay multiple versions of a slot in case any one of these versions is 
finalized by the cluster.

### Generating a Unique Identifier for Each Version of a Slot
We distinguish different versions of a slot by the ending blockhash. 
This blockhash thus needs to be "functionally unique" (meaning a malicious
leader cannot grind out a duplicate in the amount of time they have to
generate a block) across all possible shred outputs generated by a leader. 

To this end it's important to note that:

1) Two different versions of a slot must generate a different set of shreds.
Two different versions of a slot must either:

* Chain from different parents, in which case the PoH of the entries will 
  differ

* Contain different set of transactions, in which case the PoH of the 
  entries will be different

2) The resulting merkle trees of two different sets of shreds have 
"functionally unique" merkle roots.

Then if we were to generate a merkle tree out of the shred batches for a slot, 
and define the blockhash of a slot to be:

 `hash(last tick, shred merkle root)`

This blockhash should be "functionally unique".

### Generating the Merkle Tree for the Shreds
1) Leader blocks on Poh before last tick can be generated
2) Leader waits all shreds to be generated
3) Computes the Merkle tree of all the shreds
4) Inserts an entry into the Poh Stream with the Merkle root
5) Restart PoH

### Indexing Keyspaces by Blockhash
We augment all slot-related keyspaces in blocktree to include a blockhash
to support tracking multiple versions of the same slot. We outline the notable
ones below:

1) `Data` and `Erasure` keyspaces: `(slot, blockhash, shred_index)`

Two cases here:

* When leaders generate their own slot, the `blockhash` portion of the key
  is set to `Hash::default()` until the blockhash is computed at the end of 
  the slot. Then the leader will have to store a mapping from 
  `Hash::default()` to the actual blockhash in a separate area of storage 
  (another column family?). This is important in order to respond to repairs
  which will now specify a `blockhash` in addtion to a `slot` and `index`. 
  Thus if the node crashes before this mapping can be stored, then on restart
  the validator needs to recompute the blockhash in `blocktree_processor`.

*  When validators receive slots from validators, the `blockhash` portion
   of the key is set to `Hash::default()`, until they receive the entire 
   block, at whch point they follow the same procedure as the first case above
   to map `Hash::default()`to the actual blockhash.
   
   If validators see a different version of the same shred for the same slot 
   and index, then that means there has been a duplicate transmission. If such
   a conflicting version is detected before the block is completed, then 
   validators drop the block. If the block is already completed, the validators
   simply drop the duplicate and rely on the procedure outlined in the section
   `Repairing Multiple versions of the Same Slot` below.

2) `SlotMeta` keyspace: `(slot, blockhash)`
Each version of a slot will have different `consumed`, `received` and 
potentially different children as well. Thus you need a separate SlotMeta
for each version.

3) `ErasureMeta` and `IndexMeta` keyspaces: `(slot, blockhash)`
Each version of a slot can potentially have different erasure configurations,
FEC blocks etc. Each of these needs to be tracked separately

4) `Roots` keyspace: `(slot, blockhash)`
Roots will have to know which version was rooted so we can serve the correct
version through Repairman.

5) `Dead Slots` keyspace: `(slot, blockhash)`
Validators need to specify which version of a slot is dead. For instance if one
version of a slot doesn't pass entry verification, but another version does,
then the two versions must be distinguishable. 

An issue to note here is that if some version of a slot violates correctness
before the slot is finished, then the validator does not know what the ending
blockhash is and thus cannot store the slot as dead. In these cases we can drop
the entire slot and wait for repair. More details in the `Replay Failures` 
section below.

### Repairing Multiple versions of the Same Slot
Repair is augmented with a blockhash. The various types of repairs:

`Shred(slot, blockhash, index)` - Ask for a specific shred in a slot
with version `blockhash`. 

Notes:

1) Setting `blockhash` to `Hash::default()` means this validator assumes there
is only one version of this slot in the cluster and will acccept any shred
for `slot` and `index`. However, if a validator gets a mixed bag of such
shreds that do not chain, they will then drop the entire `slot`, and only
repair `slot` if some later child slot chains back to `slot` (this will be 
initiated by `Orphan` repairs below). 

2) Repair responses need to be tied to a particular blockhash. However, 
because repair responses are limited to the size of a shred, we cannot
include the blockhash in the repair response. This means we need to include 
some repair "cookie" in the request + response that maps some number of bits
to a particular `blockhash`. This breaks down if there are more versions of 
this slot than can be tracked by the number of bits allocated to the cookie.


`Orphan(slot, first_tick_hash, num_hashes)` - Ask for the last shred
of the parent of the child slot, where the child has slot equal to `slot`. 
This last shred must contain the last tick `T_p` in that parent slot such that
hashing `T_p` `num_hashes` times is equal to `first_tick_hash`.

Notes:

1) The response will include the last shred of the parent slot. From this
last shred the validator should be able to get the last tick and the 
merkle root, which can be used to compute the `blockhash`. This `blockhash`
is then used to make requests of the form `Shred(slot, blockhash, index)`
to repair the rest of this slot.

2) Also requires a cookie similar to the `Shred(slot, blockhash, index)` 
requests above.

### Chaining Multiple Versions of the Same Slot
When there are multiple versions for a slot `A`, a natural question that arises
is how validators know which version of slot `A` some descendant slot `B`
chains to. Shreds currently specify only a `parent` slot, but not which version 
of that parent. The approach to figure out chaining behavior is then:

1) Wait for all the shreds for the first entry `E_B` of slot `B` to arrive 
(Implementation can make sure first shred `S_B` always contains only a tick 
to avoid waiting for multiple shreds).
2) These shreds for slot `B` are stored under the version `Hash::default()`
(optimistically assume this child is the only version). If another conflicting
version of `B` is detected before this version is completed, we drop all the
shreds for slot `B`.
3) For all possible versions of slot `A` see which version chains to `E_B`
4) If no version of slot `A` chains, then deserialize `S_B` to find the first
tick `T_B`, then make a `Orphan(slot, T_B.hash, T_B.num_hashes)` request
to get the last shred in the version of slot `A` that chains to slot `B`.

### Replay Failures
As summarized under the `Dead Slots` keyspace in the
`Indexing the Column Families by Blockhash` section above, validators must now
account for the possibility that some versions of a slot have correctness 
issues while other versions don't.

Let `V_A` be a version of slot `A` with blockhash `B_A`.

Assume that on replay of `V_A` the validator runs into some correctness issue 
(entry verification failure, bad tick count, etc.) while replaying the entries.

Define `S` to be the set of shreds as follows:

1) On entry verification failures of entries`E1` and `E2`:

Let `S` be the set of all shreds that contain any part of `E1` and `E2`.

2) On TransactionError in some entry `E`:

Let `S` be the set of all shreds that contain any part of `E`.

3) On Blocktree inability to deserialize an entry from a set of shreds:

Let `S` be the FEC set that failed to deserialize

4) On BlockErrors (InvalidTickCount, InvalidHashCount, TrailingEntry, etc.)
on some entry `E`

Let `S` be the set of all shreds that contain any part of `E`.


Protocol: 

1) The validator queries for a merkle proof of all shreds in `S` to prove that
all the offending shreds were indeed part of the version `A` with blockhash `B_A`.

2) If the merkle proof checks out, we add `(A, B_A)` to the `Dead Slots` column
family. No further forks chaining to this slot will be played.

3) If the merkle proof instead shows that there is a different version of some
shred in `S`, that means we got maliciously sent the wrong shred for version 
`B_A`. We must then drop those wrong shreds and repair them again.