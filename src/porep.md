# Proof of replication

## Description

Filecoin proposed a version of Proof of Replication \cite{filecoinporep}. The
goal of this version is to have fast and streaming verifications of Proof of
Replication, which are enabled by keeping track of time in a Proof of History
generated sequence. Replication is not used as a consensus algorithm, but is a
useful tool to account for the cost of storing the blockchain history or state
at a high availability.


## Algorithm

As shown in Figure~\ref{fig:encrypt} CBC encryption encrypts each block of data
in sequence, using the previously encrypted block to XOR the input data.

```
\begin{figure}[h] \begin{center} \centering
\includegraphics[width=0.9\textwidth]{figures/cbc_encryption_001.png}
\caption[Fig 7]{Sequential CBC encryption\label{fig:encrypt}} \end{center}
\end{figure}
```

Each replication identity generates a key by signing a hash that has been
generated via a Proof of History sequence. This ties the key to a replicator’s
identity, and to a specific Proof of History sequence. Only specific hashes can
be selected. (See Section~\ref{hashselection} on Hash Selection)

The data set is fully encrypted block by block. Then to generate a proof, the
key is used to seed a pseudorandom number generator that selects a random 32
bytes from each block.

A merkle hash is computed with the selected PoH hash prepended to the each
slice.

```
\begin{figure}[h] \begin{center} \centering
\includegraphics[width=0.9\textwidth]{figures/por_001.png} \caption[Fig 8]{Fast
Proof of Replication\label{fig_8}} \end{center} \end{figure}
```

The root is published, along with the key, and the selected hash that was
generated. The replication node is required to publish another proof in N
hashes as they are generated by Proof of History generator, where N is
approximately \(\frac{1}{2}\) the time it takes to encrypt the data. The Proof
of History generator will publish specific hashes for Proof of Replication at
predefined periods. The replicator node must select the next published hash for
generating the proof. Again, the hash is signed, and random slices are selected
from the blocks to create the merkle root.

After a period of N proofs, the data is re-encrypted with a new CBC key.


## Verification

With N cores, each core can stream encryption for each identity. Total space
required is \(2 blocks * N cores\), since the previous encrypted block is
necessary to generate the next one. Each core can then be used to generate all
the proofs that were derived from the current encrypted block.

Total time to verify proofs is expected to be equal to the time it takes to
encrypt. The proofs themselves consume few random bytes from the block, so the
amount of data to hash is significantly lower than the encrypted block size.
The number of replication identities that can be verified at the same time is
equal to the number of available cores. Modern GPUs have 3500+ cores available
to them, albeit at \(\frac{1}{2}\)-\(\frac{1}{3}\) the clock speed of a CPU.

## Key Rotation

Without key rotation the same encrypted replication can generate cheap proofs
for multiple Proof of History sequences. Keys are rotated periodically and each
replication is re-encrypted with a new key that is tied to a unique Proof of
History sequence.

Rotation needs to be slow enough that it’s practical to verify replication
proofs on GPU hardware, which is slower per core than CPUs.


## Hash Selection

The Proof of History generator publishes a hash to be used by the entire
network for encrypting Proofs of Replication, and for using as the pseudorandom
number generator for byte selection in fast proofs.

The hash is published at a periodic counter that is roughly equal to
\(\frac{1}{2}\) the time it takes to encrypt the data set. Each replication
identity must use the same hash, and use the signed result of the hash as the
seed for byte selection, or the encryption key.

The period that each replicator must provide a proof must be smaller than the
encryption time. Otherwise the replicator can stream the encryption and delete
it for each proof.

A malicious generator could inject data into the sequence prior to this hash to
generate a specific hash. This attack is discussed more in
\ref{subsubsec:collusion}.


## Proof Validation

The Proof of History node is not expected to
validate the submitted Proof of Replication proofs. It is expected to keep
track of the number of pending and verified proofs submitted by the
replicator’s identity. A proof is expected to be verified when the replicator
is able to sign the proof by a super majority of the validators in the network.

The verifications are collected by the replicator via p2p gossip network, and
submitted as one packet that contains a super majority of the validators in the
network. This packet verifies all the proofs prior to a specific hash generated
by the Proof of History sequence, and can contain multiple replicator
identities at once.  \subsection{Attacks} \subsubsection{Spam} A malicious user
could create many replicator identities and spam the network with bad proofs.
To facilitate faster verification, nodes are required to provide the encrypted
data and the entire merkle tree to the rest of the network when they request
verification.

The Proof of Replication that is designed in this paper allows for cheap
verification of any additional proofs, as they take no additional space. But
each identity would consume 1 core of encryption time. The replication target
should be set to a maximum size of readily available cores. Modern GPUs ship
with 3500+ cores.


## Partial Erasure

A replicator node could attempt to partially erase some of the data to avoid
storing the entire state. The number of proofs and the randomness of the seed
should make this attack difficult.

For example, a user storing 1 terabyte of data erases a single byte from each 1
megabyte block. A single proof that samples 1 byte out of every megabyte would
have a likelihood of collision with any erased byte \(1 - (1-
1/1,000,0000)^{1,000,000} = 0.63\). After 5 proofs the likelihood is \(0.99\).


## Collusion with PoH generator

The signed hash is expected to be used to seed the sample. If a replicator
could select a specific hash in advance then the replicator could erase all
bytes that are not going to be sampled.

A replicator identity that is colluding with the Proof of History generator
could inject a specific transaction at the end of the sequence before the
predefined hash for random byte selection is generated. With enough cores, an
attacker could generate a hash that is preferable to the replicator’s identity.

This attack could only benefit a single replicator identity. Since all the
identities have to use the same exact hash that is cryptographically signed
with ECDSA (or equivalent), the resulting signature is unique for each
replicator identity, and collision resistant. A single replicator identity
would only have marginal gains.  \subsubsection{Denial of Service} The cost of
adding an additional replicator identity is expected to be equal to the cost of
storage. The cost of adding extra computational capacity to verify all the
replicator identities is expected to be equal to the cost of a CPU or GPU core
per replication identity.

This creates an opportunity for a denial of service attack on the network by
creating a large number of valid replicator identities.

To limit this attack, the consensus protocol chosen for the network can select
a replication target, and award the replication proofs that meet the desired
characteristics, like availability on the network, bandwidth, geolocation
etc...  

## Tragedy of Commons

The PoS verifiers could simply confirm PoRep without doing any work. The
economic incentives should be lined up with the PoS verifiers to do work, i.e.
splitting the mining payout between the PoS verifiers and the PoRep replication
nodes.

To further avoid this scenario, the PoRep verifiers can submit false proofs a
small percentage of the time. They can prove the proof is false by providing
the function that generated the false data. Any PoS verifier that confirmed a
false proof would be slashed.

